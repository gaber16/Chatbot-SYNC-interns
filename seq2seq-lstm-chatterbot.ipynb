{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-25T15:21:15.651452Z","iopub.execute_input":"2023-09-25T15:21:15.652598Z","iopub.status.idle":"2023-09-25T15:21:16.095870Z","shell.execute_reply.started":"2023-09-25T15:21:15.652541Z","shell.execute_reply":"2023-09-25T15:21:16.094668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf \nfrom tensorflow.keras import preprocessing, layers, utils, models\nimport re\nimport yaml\nimport os","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:21:16.097958Z","iopub.execute_input":"2023-09-25T15:21:16.098572Z","iopub.status.idle":"2023-09-25T15:21:24.976519Z","shell.execute_reply.started":"2023-09-25T15:21:16.098517Z","shell.execute_reply":"2023-09-25T15:21:24.975350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = '/kaggle/input/chatterbotenglish'\nfile_list = os.listdir(file_path + os.sep)\nprint(file_list)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:21:24.978119Z","iopub.execute_input":"2023-09-25T15:21:24.978944Z","iopub.status.idle":"2023-09-25T15:21:24.986541Z","shell.execute_reply.started":"2023-09-25T15:21:24.978901Z","shell.execute_reply":"2023-09-25T15:21:24.985610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"questions, answers = [], []\n\nfor file in file_list:\n    with open(file_path + os.sep + file,'rb') as file_opened:\n        doc = yaml.safe_load(file_opened)\n        conversation = doc['conversations']\n        for con in conversation:\n            if len(con) > 2:\n                questions.append(con[0])\n                answer = ''\n                for rep in con[1:]:\n                    answer += ' ' + rep\n                answers.append(answer)\n            elif len(con) > 1:\n                questions.append(con[0])\n                answers.append(con[1])\n#         print(file_opened)\nprint(f'questions length: {len(questions)}\\tanswers length: {len(answers)}')","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:21:24.988803Z","iopub.execute_input":"2023-09-25T15:21:24.989092Z","iopub.status.idle":"2023-09-25T15:21:25.300251Z","shell.execute_reply.started":"2023-09-25T15:21:24.989065Z","shell.execute_reply":"2023-09-25T15:21:25.299064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"answers[0:5]","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:21:25.301414Z","iopub.execute_input":"2023-09-25T15:21:25.301743Z","iopub.status.idle":"2023-09-25T15:21:25.309554Z","shell.execute_reply.started":"2023-09-25T15:21:25.301714Z","shell.execute_reply":"2023-09-25T15:21:25.308457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check and fix for dict in answers list as it threw an error while adding tags \n# # TypeError: can only concatenate str (not \"dict\") to str\nfor idx,n in enumerate(answers):\n    if type(n) == dict:\n        print(f'dictionary positions: {idx}')\n        print(f'dictionaries: {answers[idx]}')\n        var = ' '.join([k+v for k,v in answers[idx].items()])\n        answers[idx] = var\n        print(type(answers[idx]))\n        print(n)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:21:25.310865Z","iopub.execute_input":"2023-09-25T15:21:25.311190Z","iopub.status.idle":"2023-09-25T15:21:25.321458Z","shell.execute_reply.started":"2023-09-25T15:21:25.311164Z","shell.execute_reply":"2023-09-25T15:21:25.320284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tagging function\ndef data_tagged(data:list):\n    data_tagged = []\n    for i in range(len(data)):\n        data_tagged.append('<start>'+ data[i] + '<end>')\n    return data_tagged\n\nquestions_tagged = data_tagged(questions)\nanswers_tagged = data_tagged(answers)\nprint(questions_tagged[0:5], answers_tagged[0:5])","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:21:25.322760Z","iopub.execute_input":"2023-09-25T15:21:25.323095Z","iopub.status.idle":"2023-09-25T15:21:25.334778Z","shell.execute_reply.started":"2023-09-25T15:21:25.323067Z","shell.execute_reply":"2023-09-25T15:21:25.333631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts(questions_tagged + answers_tagged)\nvocab_size = len(tokenizer.word_index) + 1\nvocab_size","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:21:25.336022Z","iopub.execute_input":"2023-09-25T15:21:25.336375Z","iopub.status.idle":"2023-09-25T15:21:25.369650Z","shell.execute_reply.started":"2023-09-25T15:21:25.336346Z","shell.execute_reply":"2023-09-25T15:21:25.368469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vocab list and tokenization function\nvocab = []\nfor word in tokenizer.word_index:\n    vocab.append(word)\n\ndef tokenize(data):\n    data_tokenized = tokenizer.texts_to_sequences(data)\n    data_maxlen = max([len(word) for word in data_tokenized])\n    data_padded = preprocessing.sequence.pad_sequences(data_tokenized, maxlen= data_maxlen, padding= 'post')\n    return np.array(data_padded), data_maxlen","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:21:25.370853Z","iopub.execute_input":"2023-09-25T15:21:25.371171Z","iopub.status.idle":"2023-09-25T15:21:25.378540Z","shell.execute_reply.started":"2023-09-25T15:21:25.371142Z","shell.execute_reply":"2023-09-25T15:21:25.377455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenization of questions and answers for encoder and decoder inputs\nencoder_input_data, questions_maxlen = tokenize(questions_tagged)\ndecoder_input_data, answers_maxlen = tokenize(answers_tagged)\nprint(f'encoder input data shape: {encoder_input_data.shape}\\ndecoder input data shape: {decoder_input_data.shape}')\nprint(f'encoder input data : {encoder_input_data}\\ndecoder input data : {decoder_input_data}')","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:21:25.381564Z","iopub.execute_input":"2023-09-25T15:21:25.381933Z","iopub.status.idle":"2023-09-25T15:21:25.409852Z","shell.execute_reply.started":"2023-09-25T15:21:25.381901Z","shell.execute_reply":"2023-09-25T15:21:25.409034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# decoder_output -tokenization -shift sequences(teacher forcing) - padding -onehote encoding\nanswers_tokenized = tokenizer.texts_to_sequences(answers_tagged)\nfor i in range(len(answers_tokenized)):\n    answers_tokenized[i] = answers_tokenized[i][1:]\npadded_answers = preprocessing.sequence.pad_sequences(answers_tokenized, maxlen= answers_maxlen, padding='post')\nonehot_answers = utils.to_categorical(padded_answers, vocab_size)\ndecoder_output_data = np.array(onehot_answers)\ndecoder_output_data.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:21:25.410752Z","iopub.execute_input":"2023-09-25T15:21:25.411076Z","iopub.status.idle":"2023-09-25T15:21:25.932195Z","shell.execute_reply.started":"2023-09-25T15:21:25.411047Z","shell.execute_reply":"2023-09-25T15:21:25.931166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model architecture\nencoder_inputs = layers.Input(shape=(questions_maxlen,),name='encoder inputs')\nencoder_embeddings = layers.Embedding(vocab_size, 200, mask_zero= True)(encoder_inputs)\nencoder_output, state_h, state_c = layers.LSTM(256, return_state= True)(encoder_embeddings)\nencoder_states = [state_h, state_c]\n\ndecoder_inputs = layers.Input(shape=(answers_maxlen,),name='decoder inputs')\ndecoder_embeddings = layers.Embedding(vocab_size, 200, mask_zero= True) (decoder_inputs)\ndecoder_lstm = layers.LSTM(256, return_state=True, return_sequences=True)\ndecoder_outputs, _ , _ = decoder_lstm (decoder_embeddings, initial_state= encoder_states)\n\ndense = layers.Dense(vocab_size, activation='softmax')\noutputs = dense (decoder_outputs)\n\nmodel = models.Model([encoder_inputs, decoder_inputs], outputs)\nmodel.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n\nmodel.summary()\nutils.plot_model(model)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:21:25.933473Z","iopub.execute_input":"2023-09-25T15:21:25.933796Z","iopub.status.idle":"2023-09-25T15:21:28.521926Z","shell.execute_reply.started":"2023-09-25T15:21:25.933768Z","shell.execute_reply":"2023-09-25T15:21:28.520776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit([encoder_input_data, decoder_input_data], decoder_output_data, batch_size=16, verbose=1, epochs=200)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:21:28.523137Z","iopub.execute_input":"2023-09-25T15:21:28.523567Z","iopub.status.idle":"2023-09-25T15:52:46.437450Z","shell.execute_reply.started":"2023-09-25T15:21:28.523523Z","shell.execute_reply":"2023-09-25T15:52:46.436270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prediction \ndef inference():\n    encoder_model = models.Model(encoder_inputs, encoder_states)\n    \n    decoder_input_state_h = layers.Input(shape=(256, ))\n    decoder_input_state_c = layers.Input(shape=(256, ))\n    decoder_states_inputs = [decoder_input_state_h, decoder_input_state_c]\n    \n    decoder_outputs, state_h, state_c = decoder_lstm(decoder_embeddings, initial_state = decoder_states_inputs)\n    decoder_states = [state_h, state_c]\n    decoder_outputs = dense (decoder_outputs)\n    \n    decoder_model = models.Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n    \n    return encoder_model, decoder_model\n\ndef preprocessing_input(input_data:str):\n    tokens = input_data.lower().split()\n    tokens_list = []\n    for word in tokens:\n        tokens_list.append(tokenizer.word_index[word])\n    return preprocessing.sequence.pad_sequences([tokens_list], maxlen= questions_maxlen, padding= 'post')","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-09-25T15:52:46.438885Z","iopub.execute_input":"2023-09-25T15:52:46.439463Z","iopub.status.idle":"2023-09-25T15:52:46.448294Z","shell.execute_reply.started":"2023-09-25T15:52:46.439429Z","shell.execute_reply":"2023-09-25T15:52:46.447479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"enc_model, dec_model = inference()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:52:46.449415Z","iopub.execute_input":"2023-09-25T15:52:46.450218Z","iopub.status.idle":"2023-09-25T15:52:47.453465Z","shell.execute_reply.started":"2023-09-25T15:52:46.450186Z","shell.execute_reply":"2023-09-25T15:52:47.452373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"while True:\n    input_data = input('enter your text: ')\n    if input_data == 'bye':\n        print('see you later <3')\n        break\n    states_values = enc_model.predict(preprocessing_input(input_data), verbose=0)\n    empty_target_seq = np.zeros((1 , 1))\n    empty_target_seq[0, 0] = tokenizer.word_index['start']\n    stop_condition = False\n    decoded_translation = ''\n    while not stop_condition :\n        dec_outputs , h , c = dec_model.predict([empty_target_seq] + states_values, verbose=0)\n        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n        sampled_word = None\n        \n        for word , index in tokenizer.word_index.items() :\n            if sampled_word_index == index :\n                decoded_translation += f' {word}'\n                sampled_word = word\n        \n        if sampled_word == 'end' or len(decoded_translation.split()) > answers_maxlen:\n            stop_condition = True\n            \n        empty_target_seq = np.zeros((1 , 1))  \n        empty_target_seq[0 , 0] = sampled_word_index\n        states_values = [h , c] \n    \n    print()\n    decoded_translation = decoded_translation.split(' end')[0]\n    print(f'Bot: {decoded_translation}')\n    print('-'*25)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T15:52:47.454756Z","iopub.execute_input":"2023-09-25T15:52:47.455158Z","iopub.status.idle":"2023-09-25T15:58:01.402659Z","shell.execute_reply.started":"2023-09-25T15:52:47.455129Z","shell.execute_reply":"2023-09-25T15:58:01.401153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}